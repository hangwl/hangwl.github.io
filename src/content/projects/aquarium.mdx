---
title: "Aquarium"
description: "Empowering Southeast Asian languages."
date: "2025-06-05"
tags: ["Next.js", "Python", "PostgreSQL", "FastAPI", "LangGraph"]
image: "/images/aquarium.svg"
website: "https://aquarium.sea-lion.ai/"
status: "completed"
featured: true
order: 1
---

# Introduction

Aquarium is a platform that was developed to address challenges related to the accessibility of language datasets specifically within Southeast Asia. 
The region is incredibly linguistically diverse and is home to over 1,200 languages and dialects. 
With recent developments in large language models, leaders have recognized the importance of regional collaboration and 
providing incentives for the community to contribute datasets that increase Southeast Asian representation. 

As an AI Apprentice, I was given the opportunity to work on the beta version of the Aquarium project for a period of 7 months alongside 2 mentors and 2 other apprentices. 
In AI Singapore, apprentices are presented with a real-world truth. That is, there will not always be a "best-practice" to follow for every project. 
Thus, it is up to the apprentices to innovate and come up with solutions that are tailored to their project's needs.

## Project Scope

The initial scope of the project presented to the apprentices was truthfully vague. 
There were 2 main objectives:
1. Implement a chatbot that is able to help users search for SEA datasets and provide insights. 
2. Implement a contribution pipeline for users to contribute datasets. 

## Data

To get things started, we were tasked to explore and analyze the data in the [SEACrowd Catalogue](https://seacrowd.github.io/seacrowd-catalogue/). 
The public spreadsheet contains a collection of dataset metadata contributed by individuals from the [SEACrowd community](https://seacrowd.github.io/). 
The primary goal of exploring the data was to understand how various metadata fields may be useful for the chatbot we were trying to build.

![SEACrowd Catalogue CSV](/images/seacrowd-catalogue-csv.png)

### Metadata Fields

Within the spreadsheet, many metadata fields are available. 

The following fields were identified as important metadata fields that provide the most qualitative context:
1. Dataset Name
2. Dataset Description
3. Dataset Language(s)
4. Dataset Task(s)
5. Dataset Modality
6. Dataset Domain(s)

Separately, several quantitative fields were also identified to be important:
1. Dataset Samples
2. Dataset Download Size

Various other metadata fields were also identified to be potentially important for the sake of data provenance, as well as to indicate permissibility.

### Data Modeling

To support the chatbot, the data needed to be modeled in a way to enable semantic and keyword search capabilities. 

#### Document Model

In [LangChain](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html), the Document class can be used to store a piece of text and associated its metadata. 
This is done via the `page_content` and optional `metadata` attributes. 

```python title="Document Model Example"
from langchain_core.documents import Document

document = Document(
    page_content="The Abui Wordnet dataset is a linguistic resource ...",
    metadata={
        "name": "Abui Wordnet",
        "languages": ["abz"],
        "tasks": ["evaluation"],
        "modalities": ["text"]
        "domains": ["linguistic", "wordnet"]
        "samples": 3606,
        "download_size": "70470",
    }
)
```

While this document model is potentially sufficient to support a chatbot that is capable of hybrid search, its design may be too simplistic and difficult to scale. 
Diving deeper into the data, we realized that datasets may originate from multiple contributors, as well as contain nested dataset sub-collections.
This is something we needed to address by designing a different data model.

#### Relational Model

The following relational model is a simplified version of the relational data model that was designed. 

![Aquarium Relational Model](/images/aquarium_relational_model.svg)

Using the [pgvector extension](https://github.com/pgvector/pgvector) in PostgreSQL, we were able to serve the following data model in a highly scalable, vector-capable relational database.

## Chatbot

Agentic workflows enable the automation of tasks that were previously impossible. 
However, building systems that can reliably execute specific tasks is a challenge. 
This is especially noticeable when systems swap between different language models that behave differently.
A unique system prompt needs to be optimized for each injected LLM in production.
Thus, it is important to ensure that the initial core LLM workflow that is designed is not overly complex.

### Core Concepts

#### Control Flow

[LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/) supports agentic LLM workflows that are able to autonomously determine the control flow of an application. 

![Control Flow Agents](/images/LangChain_Academy_-_Introduction_to_LangGraph_-_Motivation-09.png)

Generally, the more control an LLM agent has over the control flow of an application, the less reliable the system becomes. 

![Reliability Tradeoffs](/images/LangChain_Academy_-_Introduction_to_LangGraph_-_Motivation-10.png)

However in LangGraph, memory can be shared between agent invocations via persistent graph states. 
This ensures that the overall context of a running conversation is well-preserved so as to improve the reliability of highly autonomous agent workflows.

![Reliability Tradeoffs](/images/LangChain_Academy_-_Introduction_to_LangGraph_-_Motivation-11.png)

#### ReAct

The [Reason and Act (ReAct)](https://arxiv.org/abs/2210.03629) framework is a popular general purpose agent architecture that leverages 
the ability of LLMs to [plan](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#planning), use [tools](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#tool-calling), and act on observations to achieve a certain goal.
The process is autonomous and a LLM is capable of dynamic updating its strategy based on the observations it receives.

![Simple ReAct Graph](/images/langgraph-react-agent_16_0.png)

### Requirements

The main requirement of the Aquarium chatbot is that it should be able to match a user's query to the most relevant dataset available. 
Given that the Aquarium platform is meant to serve users of various native tongues, the chatbot should also be able to understand non-English queries, and respond appropriately. 

### Agents

#### Orchestrator Agent

The orchestrator agent is the main agent that is responsible for orchestrating the entire chatbot workflow as well as supervising and delegating tasks to other agents. 
It serves as the brain for understanding user intent, managing conversation flow, and deciding what information is needed for other agents to complete their tasks.

As the orchestrator, it is given access to the following tools that should be called in order:

##### Translation Tool

The translation tool is essential to ensure that user queries are first standardized to English. 
This is crucial given that almost all documents contain English metadata. 
This ensures that query embeddings and document embeddings are aligned, and do not require the use of a cross-lingual embedding model.

##### Query Expander Tool

If a user's query is too sparse or ambiguous, the orchestrator agent can decide to call the query expander tool to generate a more detailed query that improves retrieval recall performance. 
This works by adding synonyms or related terms, crafting more precise queries, or creating multiple query variants to cast a wider net for the initial retrieval.

> It is important to note that over-expansion may lead to a decrease in retrieval precision, as the original intent of a query may become distorted.

#### Retrieval Agent

After initial query preprocessing, the retrieval agent should be invoked if a data retrieval action is needed. 
As the "Data Expert", it is responsible for all operations to retrieve data from our unified PostgreSQL + `pgvector` database.

To accomplish data retrieval goals, the retrieval agent is given access to the following tools that should be called in order:

##### Query Embedder Tool

The purpose of the query embedder tool is to convert an input query into a vector embedding that can be used to perform vector search. 
By comparing cosine similarity scores between query embeddings and document embeddings, the most relevant datasets can be retrieved.

##### Text-to-SQL Tool

The Text-to-SQL tool can be used to convert natural language queries into SQL queries aligned with the schema of the database. 
Given that database entities can be nested and contain complex relationships, this tool can be very powerful when it works. 
It allows the retrieval agent to perform more precise, filtered data retrieval operations.

> It is important to note that presently, the performance of Text-to-SQL tools fail to meet expectations when dealing with complex databases. 
However, models are continuously improving and we can expect exponential improvements in the near future. 

##### Database Retrieval Tool

The database retrieval tool is used to execute any generated SQL query against the PostgreSQL database.

##### Reranker Tool (Optional)

Finally, a reranker tool that leverages a reranker model or another LLM as a judge can be used by the retrieval agent to refine and reorder retrieval results. 

> For a small database, a reranker tool may not be necessary. 
However as data grows, the addition of a reranking layer may be useful to improve retrieval precision. 

#### Response Generation Agent

After sufficient information is retrieved via the retrieval agent, and the orchestrator agent decides that sufficient context has been acquired, 
the response generation agent should be invoked to generate an appropriate response to the initial user's query. 
This may necessitate calling the `translation tool` once more to translate the response back to the user's original language, 
or the language the response was requested to be in.

In its system prompt, the final response is instructed to be in markdown format which is flexible and highly portable.

## Contribution Pipeline

> WIP

## Me and the Boys

![Me and the boys at swensens unlimited](/images/aquarium-swensens.png)
