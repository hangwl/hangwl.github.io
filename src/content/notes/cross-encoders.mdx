---
title: "Cross-Encoders"
links:
 - 'reranking'
---

## Introduction

RAG systems typically rely on Bi-Encoder embedding models to generate and compare between document and query vectors for semantic search. 

To further improve search results, **Cross-Encoder** models can be used instead. 
They are **specialized embedding models** that accept **prompt-document pairs** as input, returning the **relevancy scores** for each pair.

>When a knowledge base contains large number of documents, finding the relevancy score for every concatenated prompt and document is not a scalable solution. 
However in small knowledge bases, Cross-Encoders can significantly improve the quality of search results. 

![Cross-Encoder](/images/e7iq5MmR5x.png)

## Contextualized Late Interaction Over BERT (ColBERT)

**ColBERT** an architecture that takes the following approach that addresses the **scalability** and **latency limitations** of Bi-Encoders. 

Instead of generating one vector for each prompt and document, every token is vectorized. 
Each prompt vector tries to find its most similar document vector. 
The highest relevancy scores for each prompt-document token pair are summed to form the **MaxSim** score for a document. 
Repeating this process for all documents in a database allows us to rank and retrieve the most relevant documents for a given prompt. 

![MaxSim](/images/arybRTzGQo.png)

>While ColBERT is designed to be scalable and reasonably fast, note that it requires significantly more storage for every vectorized token. 
Generally, Bi-Encoder architectures should be preferred as the default semantic search option unless a project requires higher precision, or greater contextual understanding e.g. in Legal or Medical fields.