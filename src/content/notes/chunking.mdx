---
title: "Chunking"
---

## Introduction

Chunking is the process of breaking down documents into **smaller segments**. 

With a good chunking strategy, we can improve the overall retrieval quality of a RAG system. 
Mainly, chunking can allows us to **more precisely represent** a long document over multiple vectors instead of just a single compressed vector. 
Since each vector represents a different document chunk, we can also **focus on the specific segment** of a document that is most relevant to a query instead of retrieving the entire document.

![Why Chunk Documents](/images/zAna6yWLBd.png)

## Advanced Chunking

**Basic chunking** techniques focus on adjusting **chunk size** and **chunk overlap**.
However, we cannot always guarantee that high quality document chunks are returned even after optimizing these factors. 
Thus, more advanced chunking strategies are important. 

### Semantic Chunking

Semantic chunking is a chunking strategy that attempts to break down documents into segments that contain **semantically related sentences**. 
- Documents are processed sentence by sentence.
- For every subsequent sentence, we calculate the semantic similarity between its vector and the vector of the current chunk. 
- If the semantic similarity score is above a certain threshold, the sentence is added to the chunk and we move on to compare the next sentence.
- Otherwise, we start a new chunk with the current sentence and repeat the process. 

![Semantic Chunking Visualization](/images/mqjP1E534K.png)

![Pros and Cons](/images/wTCrTgCeTc.png)

### Language Based Chunking

Language based chunking uses **LLMs** to chunk documents. 
This is done by directly providing **prompt instructions** on the types of chunks we want to extract from a document. 

>This is becoming increasingly more feasible as LLMs become cheaper.

![Language Based Chunking](/images/UYrEBfu9Wx.png)

### Context-Aware Chunking

This approach tries to make use of LLMs to **append additional context** to each chunk. 
The added context may **improve** the vectorization process and increase overall **search relevance** scores. 
Additionally, the added context may also be useful during the **response generation** step for the underlying LLM. 

![Context Aware Chunking](/images/9ROHXfA1tQ.png)